101000,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFileBasedIPList.java,36,,"    removeFile(""ips.txt"");"
101001,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestFileBasedIPList.java,47,,"    String[] ips = {""10.119.103.112"", ""10.221.102.0/23""};"
101002,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGSet.java,53,,"        LightWeightGSet.LOG.info(""GOOD: getting "" + e, e);"
101003,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGSet.java,211,,"    println(""DONE "" + test.stat());"
101004,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,61,,"    args[0] = ""-files"";"
101005,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,66,,"    String files = conf.get(""tmpfiles"");"
101006,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,280,,"    expectedMap.put(""key1"", ""value1"");"
101007,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,280,,"    expectedMap.put(""key1"", ""value1"");"
101008,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,283,,"    args = new String[]{""-fs"", ""hdfs://somefs/"", ""-Dkey1=value1"", ""arg1""};"
101009,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,283,,"    args = new String[]{""-fs"", ""hdfs://somefs/"", ""-Dkey1=value1"", ""arg1""};"
101010,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,283,,"    args = new String[]{""-fs"", ""hdfs://somefs/"", ""-Dkey1=value1"", ""arg1""};"
101011,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,299,,"        ""-fs"", ""someother"", ""-D"", ""key2"", ""value2"", ""arg1"", ""arg2""};"
101012,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java,299,,"        ""-fs"", ""someother"", ""-D"", ""key2"", ""value2"", ""arg1"", ""arg2""};"
101013,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestHostsFileReader.java,67,,"    efw.write(""#DFS-Hosts-excluded\n"");"
101014,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestHostsFileReader.java,76,,"    ifw.write(""#Hosts-in-DFS\n"");"
101015,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestHostsFileReader.java,93,,"    assertTrue(hfp.getHosts().contains(""somehost5""));"
101016,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestIndexedSort.java,40,,"    assertTrue(Arrays.toString(values) + ""\ndoesn't match\n"" +"
101017,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestLightWeightCache.java,85,,"    println(""DONE "" + test.stat());"
101018,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,52,,"    assertTrue(ml.includes(""10.119.103.112""));"
101019,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,53,,"    assertTrue(ml.includes(""1.2.3.4""));"
101020,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,65,,"    assertFalse(ml.includes(""10.119.103.111""));"
101021,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,89,,"    Mockito.when(addressFactory.getByName(""host4"")).thenReturn(addressHost4);"
101022,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,98,,"    assertFalse(ml.includes(""1.2.3.5""));"
101023,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,163,,"    assertFalse(ml.includes(""10.221.255.255""));"
101024,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,164,,"    assertTrue(ml.includes(""10.222.0.0"")); "
101025,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,169,,"    assertTrue(ml.includes(""10.222.255.255""));"
101026,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,170,,"    assertFalse(ml.includes(""10.223.0.0""));"
101027,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,172,,"    assertTrue(ml.includes(""10.241.23.0""));"
101028,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java,175,,"    assertTrue(ml.includes(""10.241.23.255""));"
101029,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestRunJar.java,73,,"    jstream.putNextEntry(new ZipEntry(""foobar.txt""));"
101030,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringInterner.java,38,,"    String literalABC = ""ABC"";"
101031,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestVersionUtil.java,29,,"    assertEquals(0, VersionUtil.compareVersions(""2.0.0"", ""2.0.0""));"
101032,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestVersionUtil.java,35,,"    assertEquals(0, VersionUtil.compareVersions(""1"", ""1.0""));"
101033,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestVersionUtil.java,36,,"    assertEquals(0, VersionUtil.compareVersions(""1"", ""1.0.0""));"
101034,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestVersionUtil.java,88,,"    assertExpectedValues(""1.0.0"", ""1.0.1-SNAPSHOT"");"
101035,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java,87,,"    assertTrue(outputArgs[0].equals(""-rwx------""));"
101036,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java,156,,"    chmod(""700"", a);"
101037,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java,419,,"      Shell.execCommand(Shell.WINUTILS, ""symlink"", link, target);"
101038,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java,468,,"        ""readlink"","
101039,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestWinUtils.java,483,,"      fail(""Failed to get Shell.ExitCodeException when reading bad symlink"");"
101040,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/bloom/BloomFilterCommonTester.java,155,,"        assertTrue("" might contain key error "","
101041,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/bloom/BloomFilterCommonTester.java,360,,"          Assert.assertTrue("" filter might contains "" + i,"
101042,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMS.java,176,,"  public Response deleteKey(@PathParam(""name"") final String name)"
101043,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMS.java,291,,"  @Path(KMSRESTConstants.KEY_RESOURCE + ""/{name:.*}/"" +"
101044,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMS.java,344,,"      @PathParam(""versionName"") final String versionName) throws Exception {"
101045,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/main/java/org/apache/hadoop/crypto/key/kms/server/KMSServerJSONUtils.java,36,,"  @SuppressWarnings(""unchecked"")"
101046,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,181,,"    conf.set(""hadoop.kms.authentication.type"", ""simple"");"
101047,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,238,,"      options.put(""useKeyTab"", ""true"");"
101048,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,269,,"    principals.add(""HTTP/localhost"");"
101049,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,270,,"    principals.add(""client"");"
101050,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,314,,"      conf.set(""hadoop.security.authentication"", ""kerberos"");"
101051,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,314,,"      conf.set(""hadoop.security.authentication"", ""kerberos"");"
101052,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,335,,"      conf.set(""hadoop.kms.authentication.kerberos.keytab"","
101053,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,337,,"      conf.set(""hadoop.kms.authentication.kerberos.principal"", ""HTTP/localhost"");"
101054,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,338,,"      conf.set(""hadoop.kms.authentication.kerberos.name.rules"", ""DEFAULT"");"
101055,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,338,,"      conf.set(""hadoop.kms.authentication.kerberos.name.rules"", ""DEFAULT"");"
101056,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,419,,"        options.setCipher(""AES/CTR/NoPadding"");"
101057,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMS.java,598,,"        kpdte.addDelegationTokens(""foo"", credentials);"
101058,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java,86,,"    kmsAudit.ok(luser, KMSOp.DECRYPT_EEK, ""k1"", ""testmsg"");"
101059,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-minikdc/src/test/java/org/apache/hadoop/minikdc/TestMiniKdc.java,99,,"      options.put(""useKeyTab"", ""true"");"
101060,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java,262,,"              "", allowing client '"" + address + ""', '"" + hostname + ""'"");"
101061,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-nfs/src/main/java/org/apache/hadoop/nfs/NfsExports.java,377,,"        LOG.debug(""Using match all for '"" + host + ""' and "" + privilege);"
101062,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/nfs/nfs3/TestIdUserGroup.java,89,,"        + "" | cut -d: -f1,3"";"
101063,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/nfs/nfs3/TestIdUserGroup.java,101,,"    assertEquals(""hdfs"", uMap.get(10));"
101064,./TargetProjects/hadoop-common/hadoop-common-project/hadoop-nfs/src/test/java/org/apache/hadoop/oncrpc/TestRpcCallCache.java,44,,"    new RpcCallCache(""test"", 0);"
101065,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/client/HttpFSFileSystem.java,117,,"  public static final String RENAME_JSON = ""boolean"";"
101066,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java,186,,"    @SuppressWarnings({""unchecked""})"
101067,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java,251,,"  @SuppressWarnings({""unchecked"", ""rawtypes""})"
101068,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java,192,,"  @Path(""{path:.*}"")"
101069,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java,194,,"  public Response get(@PathParam(""path"") String path,"
101070,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/HttpFSServer.java,222,,"        AUDIT_LOG.info(""[{}]"", path);"
101071,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/server/Server.java,360,,"    log.info(""    Version           : {}"", serverInfo.getProperty(name + "".version"", ""undef""));"
101072,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/lib/service/instrumentation/InstrumentationService.java,65,,"  @SuppressWarnings(""unchecked"")"
101073,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/BaseTestHttpFSWith.java,82,,"    Assert.assertTrue(new File(homeDir, ""conf"").mkdir());"
101074,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/BaseTestHttpFSWith.java,150,,"    Path path = new Path(getProxiedFSTestDir(), ""foo.txt"");"
101075,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/BaseTestHttpFSWith.java,501,,"      final String name1 = ""user.a1"";"
101076,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/BaseTestHttpFSWith.java,503,,"      final String name2 = ""user.a2"";"
101077,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/BaseTestHttpFSWith.java,505,,"      final String name3 = ""user.a3"";"
101078,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/client/BaseTestHttpFSWith.java,507,,"      final String name4 = ""trusted.a1"";"
101079,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestCheckUploadContentTypeFilter.java,34,,"    test(""PUT"", HttpFSFileSystem.Operation.CREATE.toString(), ""application/octet-stream"", true, false);"
101080,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestCheckUploadContentTypeFilter.java,44,,"    test(""PUT"", HttpFSFileSystem.Operation.CREATE.toString(), ""plain/text"", false, false);"
101081,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java,124,,"    Assert.assertTrue(new File(homeDir, ""conf"").mkdir());"
101082,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java,272,,"    conn.setRequestMethod(""PUT"");"
101083,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java,413,,"    String statusJson = getStatus(""/perm/none"", ""GETFILESTATUS"");"
101084,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java,450,,"    String statusJson = getStatus(path, ""GETXATTRS"");"
101085,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java,521,,"    Assert.assertEquals(-1, statusJson.indexOf(""aclBit""));"
101086,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServer.java,526,,"    statusJson = getStatus(path, ""GETACLSTATUS"");"
101087,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerNoACLs.java,101,,"    Assert.assertTrue(new File(homeDir, ""conf"").mkdir());"
101088,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSServerNoXAttrs.java,102,,"    Assert.assertTrue(new File(homeDir, ""conf"").mkdir());"
101089,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/fs/http/server/TestHttpFSWithKerberos.java,69,,"    Assert.assertTrue(new File(homeDir, ""conf"").mkdir());"
101090,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,54,,"    Server server = new Server(""server"", getAbsolutePath(""/a""),"
101091,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,63,,"    assertEquals(server.getPrefixedName(""name""), ""server.name"");"
101092,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,63,,"    assertEquals(server.getPrefixedName(""name""), ""server.name"");"
101093,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,99,,"  @TestException(exception = ServerException.class, msgRegExp = ""S01.*"")"
101094,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,102,,"    File homeDir = new File(TestDirHelper.getTestDir(), ""home"");"
101095,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,104,,"    conf.set(""server.services"", TestService.class.getName());"
101096,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,110,,"  @TestException(exception = ServerException.class, msgRegExp = ""S02.*"")"
101097,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,127,,"    assertTrue(new File(homeDir, ""log"").mkdir());"
101098,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,128,,"    assertTrue(new File(homeDir, ""temp"").mkdir());"
101099,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,143,,"    File configDir = new File(homeDir, ""conf"");"
101100,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,326,,"      LIFECYCLE.add(""serverStatusChange"");"
101101,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,392,,"    assertEquals(server.getConfig().get(""testserver.a""), ""default"");"
101102,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,665,,"    assertEquals(ORDER.get(4), ""s3.destroy"");"
101103,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServer.java,666,,"    assertEquals(ORDER.get(5), ""s1.destroy"");"
101104,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServerConstructor.java,39,,"      {""server"", null, null, null, null, null},"
101105,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServerConstructor.java,41,,"      {""server"", ""foo"", null, null, null, null},"
101106,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/server/TestServerConstructor.java,42,,"      {""server"", ""/tmp"", null, null, null, null},"
101107,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/hadoop/TestFileSystemAccessService.java,61,,"    hadoopConf.set(""foo"", ""FOO"");"
101108,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/hadoop/TestFileSystemAccessService.java,74,,"    conf.set(""server.services"", services);"
101109,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/hadoop/TestFileSystemAccessService.java,75,,"    Server server = new Server(""server"", dir, dir, dir, dir, conf);"
101110,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/hadoop/TestFileSystemAccessService.java,92,,"    conf.set(""server.hadoop.authentication.type"", ""kerberos"");"
101111,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/hadoop/TestFileSystemAccessService.java,110,,"    conf.set(""server.hadoop.authentication.kerberos.keytab"", ""/tmp/foo"");"
101112,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/hadoop/TestFileSystemAccessService.java,265,,"    conf.set(""server.hadoop.filesystem.cache.purge.timeout"", ""0"");"
101113,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/instrumentation/TestInstrumentationService.java,285,,"        return ""foo"";"
101114,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/instrumentation/TestInstrumentationService.java,348,,"    assertNotNull(snapshot.get(""jvm""));"
101115,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/instrumentation/TestInstrumentationService.java,349,,"    assertNotNull(snapshot.get(""counters""));"
101116,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/service/instrumentation/TestInstrumentationService.java,352,,"    assertNotNull(snapshot.get(""samplers""));"
101117,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/servlet/TestMDCFilter.java,48,,"    Mockito.when(request.getMethod()).thenReturn(""METHOD"");"
101118,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/servlet/TestMDCFilter.java,49,,"    Mockito.when(request.getPathInfo()).thenReturn(""/pathinfo"");"
101119,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/servlet/TestMDCFilter.java,59,,"        assertEquals(MDC.get(""hostname""), null);"
101120,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/servlet/TestMDCFilter.java,60,,"        assertEquals(MDC.get(""user""), null);"
101121,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/servlet/TestMDCFilter.java,61,,"        assertEquals(MDC.get(""method""), ""METHOD"");"
101122,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/servlet/TestMDCFilter.java,62,,"        assertEquals(MDC.get(""path""), ""/pathinfo"");"
101123,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestCheck.java,34,,"    assertEquals(Check.notNull(""value"", ""name""), ""value"");"
101124,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestCheck.java,34,,"    assertEquals(Check.notNull(""value"", ""name""), ""value"");"
101125,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestCheck.java,121,,"    assertEquals(Check.gt0(120, ""test""), 120);"
101126,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestConfigurationUtils.java,59,,"    srcConf.set(""testParameter1"", ""valueFromSource"");"
101127,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestConfigurationUtils.java,59,,"    srcConf.set(""testParameter1"", ""valueFromSource"");"
101128,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestConfigurationUtils.java,60,,"    srcConf.set(""testParameter2"", ""valueFromSource"");"
101129,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestConfigurationUtils.java,63,,"    targetConf.set(""testParameter3"", ""valueFromTarget"");"
101130,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestConfigurationUtils.java,80,,"    targetConf.set(""testParameter2"", ""originalValueFromTarget"");"
101131,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/lib/util/TestConfigurationUtils.java,99,,"    conf.set(""b"", ""${a}"");"
101132,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/test/KerberosTestUtils.java,82,,"      options.put(""useKeyTab"", ""true"");"
101133,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/test/TestHFSTestCase.java,146,,"      OutputStream os = fs.create(new Path(TestHdfsHelper.getHdfsTestDir(), ""foo""));"
101134,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-httpfs/src/test/java/org/apache/hadoop/test/TestHTestCase.java,128,,"      resp.getWriter().write(""foo"");"
101135,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/mount/RpcProgramMountd.java,99,,"      LOG.debug(""MOUNT NULLOP : "" + "" client: "" + client);"
101136,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java,287,,"      LOG.warn(""Exception "", r);"
101137,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java,304,,"      LOG.error(""Can't get path for fileId:"" + handle.getFileId());"
101138,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java,479,,"        LOG.info(""Can't get path for dir fileId:"" + dirHandle.getFileId());"
101139,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/RpcProgramNfs3.java,1069,,"          LOG.info(""Can't get postOpDirAttr for "" + dirFileIdPath, e);"
101140,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/WriteManager.java,248,,"        LOG.error(""Should not get commit return code:"" + ret.name());"
101141,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestOpenFileCtxCache.java,51,,"    OpenFileCtx context1 = new OpenFileCtx(fos, attr, ""/dumpFilePath"","
101142,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestReaddir.java,113,,"    DFSTestUtil.createFile(hdfs, new Path(testdir + ""/f2""), 0, (short) 1, 0);"
101143,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestReaddir.java,132,,"        securityHandler, new InetSocketAddress(""localhost"", 1234));"
101144,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java,142,,"    HdfsFileStatus status = nn.getRpcServer().getFileInfo(""/tmp/bar"");"
101145,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java,151,,"        new InetSocketAddress(""localhost"", 1234));"
101146,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java,152,,"    assertEquals(""Incorrect return code"", Nfs3Status.NFS3ERR_ACCES,"
101147,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java,169,,"    xdr_req.writeString(""bar"");"
101148,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java,241,,"    xdr_req.writeString(""fubar"");"
101149,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestRpcProgramNfs3.java,248,,"    assertEquals(""Incorrect return code:"", Nfs3Status.NFS3_OK,"
101150,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestWrites.java,313,,"        System.getProperty(""user.name""));"
101151,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs-nfs/src/test/java/org/apache/hadoop/hdfs/nfs/nfs3/TestWrites.java,347,,"          securityHandler, new InetSocketAddress(""localhost"", 1234));"
101152,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/src/test/java/org/apache/hadoop/contrib/bkjournal/TestCurrentInprogress.java,136,,"    ci.update(""myInprogressZnode"");"
101153,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java,374,,"            append("", block="").append(block)."
101154,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java,417,,"            + "" = "" + useLegacyBlockReaderLocal);"
101155,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java,64,,"  public static final String  DFS_CLIENT_CONTEXT_DEFAULT = ""default"";"
101156,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSConfigKeys.java,132,,"  public static final String  DFS_NAMENODE_HTTP_ADDRESS_DEFAULT = ""0.0.0.0:"" + DFS_NAMENODE_HTTP_PORT_DEFAULT;"
101157,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java,749,,"            + getCurrentBlock() + "" from "" + currentNode"
101158,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java,759,,"              + getCurrentBlock() + "" of "" + src + "" from """
101159,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSOutputStream.java,475,,"            DFSClient.LOG.warn(""Caught exception "", e);"
101160,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java,280,,"            "" a symlink to a non-DistributedFileSystem: "" + f + "" -> "" + p);"
101161,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java,420,,"  @SuppressWarnings(""deprecation"")"
101162,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java,1032,,"          throw new FileNotFoundException(""File does not exist: "" + p);"
101163,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java,1330,,"          throw new UnsupportedOperationException(""Cannot perform snapshot"""
101164,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DistributedFileSystem.java,1331,,"              + "" operations on a symlink to a non-DistributedFileSystem: """
101165,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/LeaseRenewer.java,295,,"                LOG.debug(""Lease renewer daemon for "" + clientsString()"
101166,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/LeaseRenewer.java,296,,"                    + "" with renew id "" + id + "" started"");"
101167,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/NameNodeProxies.java,145,,"  @SuppressWarnings(""unchecked"")"
101168,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java,584,,"          ""response"");"
101169,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java,268,,"      throw new InvalidToken(""Block token with "" + id.toString()"
101170,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java,519,,"      System.out.println(e + "".  Exiting ..."");"
101171,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java,94,,"    assert this.triplets != null : ""BlockInfo is not initialized"";"
101172,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java,95,,"    assert index >= 0 && index*3 < triplets.length : ""Index is out of bound"";"
101173,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java,298,,"          + "" = "" + minR + "" <= 0"");"
101174,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java,1176,,"    blockLog.info(""BLOCK* invalidateBlock: "" + b + "" on "" + dn);"
101175,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java,1842,,"          + b + "" on "" + node + "" size "" + b.getNumBytes()"
101176,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java,190,,"          ""toAdd: "" + Joiner.on("","").useForNull(""<default>"").join(toAdd) +"
101177,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java,295,,"        + "" CTime = "" + this.getCTime()"
101178,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java,218,,"              "" while receiving block "" + block + "" from "" + inAddr);"
101179,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java,386,,"      LOG.warn(""Slow flushOrSync took "" + duration + ""ms (threshold="""
101180,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java,467,,"        : dnR + "" Served block "" + block + "" to "" +"
101181,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java,477,,"        String msg = ""opReadBlock "" + block + "" received exception "" + e; "
101182,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java,963,,"    updateCurrentThreadName(""Replacing block "" + block + "" from "" + delHint);"
101183,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java,330,,"          LOG.warn(""Failed to cache "" + key + "": could not reserve "" + length +"
101184,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java,429,,"      throw new IOException(""Block "" + b + "" is not valid."");"
101185,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java,540,,"          + "" from "" + srcmeta + "" to "" + dstmeta, e);"
101186,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java,540,,"          + "" from "" + srcmeta + "" to "" + dstmeta, e);"
101187,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java,1240,,"          LOG.info(""Failed to delete replica "" + invalidBlks[i]"
101188,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java,1311,,"          LOG.warn(""Failed to cache block with id "" + blockId + "", pool "" +"
101189,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java,1666,,"        + "", replica="" + replica);"
101190,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java,329,,"        throw new IOException(""Failed to delete "" + rbwDir);"
101191,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java,114,,"                bpid + "" on volume "" + v + ""..."");"
101192,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java,115,,"          ""Packet Ack RTT in ns"", ""ops"", ""latency"", interval);"
101193,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java,115,,"          ""Packet Ack RTT in ns"", ""ops"", ""latency"", interval);"
101194,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java,122,,"    final DataNode datanode = (DataNode) context.getAttribute(""datanode"");"
101195,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/resources/DatanodeWebHdfsMethods.java,143,,"  @Consumes({""*/*""})"
101196,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java,537,,"      LOG.warn(""addDirective of "" + info + "" failed: "", e);"
101197,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CachedBlock.java,170,,"        ""in the list."");"
101198,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java,439,,"          +src+"" to ""+dst);"
101199,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java,516,,"      NameNode.stateChangeLog.warn(""DIR* FSDirectory.unprotectedRenameTo: """
101200,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java,517,,"                                   +""failed to rename ""+src+"" to ""+dst+ "
101201,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java,291,,"        ""Bad state: %s"", state);"
101202,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,608,,"      builder.append("", path="");"
101203,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,631,,"      builder.append("", opCode="");"
101204,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,633,,"      builder.append("", txid="");"
101205,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,641,,"      XMLUtils.addSaxString(contentHandler, ""LENGTH"","
101206,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,643,,"      XMLUtils.addSaxString(contentHandler, ""INODEID"","
101207,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,645,,"      XMLUtils.addSaxString(contentHandler, ""PATH"", path);"
101208,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,646,,"      XMLUtils.addSaxString(contentHandler, ""REPLICATION"","
101209,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,648,,"      XMLUtils.addSaxString(contentHandler, ""MTIME"","
101210,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,650,,"      XMLUtils.addSaxString(contentHandler, ""ATIME"","
101211,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,679,,"      if (st.hasChildren(""BLOCK"")) {"
101212,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1084,,"          throw new IOException(""Incorrect data format. """
101213,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1116,,"      builder.append("", timestamp="");"
101214,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1132,,"      XMLUtils.addSaxString(contentHandler, ""TIMESTAMP"","
101215,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1249,,"      XMLUtils.addSaxString(contentHandler, ""SRC"", src);"
101216,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1250,,"      XMLUtils.addSaxString(contentHandler, ""DST"", dst);"
101217,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1569,,"      XMLUtils.addSaxString(contentHandler, ""GENSTAMP"","
101218,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1675,,"      XMLUtils.addSaxString(contentHandler, ""BLOCK_ID"","
101219,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1739,,"      XMLUtils.addSaxString(contentHandler, ""MODE"","
101220,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1816,,"        XMLUtils.addSaxString(contentHandler, ""USERNAME"", username);"
101221,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1819,,"        XMLUtils.addSaxString(contentHandler, ""GROUPNAME"", groupname);"
101222,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,1875,,"      XMLUtils.addSaxString(contentHandler, ""NSQUOTA"","
101223,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,2234,,"      XMLUtils.addSaxString(contentHandler, ""VALUE"", value);"
101224,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,2554,,"      XMLUtils.addSaxString(contentHandler, ""EXPIRY_TIME"","
101225,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,2560,,"          ""DELEGATION_TOKEN_IDENTIFIER"").get(0));"
101226,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,2875,,"      XMLUtils.addSaxString(contentHandler, ""SNAPSHOTROOT"", snapshotRoot);"
101227,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,2876,,"      XMLUtils.addSaxString(contentHandler, ""SNAPSHOTNAME"", snapshotName);"
101228,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,4219,,"      contentHandler.startElement("""", """", ""ENTRY"", new AttributesImpl());"
101229,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,4223,,"        XMLUtils.addSaxString(contentHandler, ""NAME"", e.getName());"
101230,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogOp.java,4250,,"      contentHandler.startElement("""", """", ""XATTR"", new AttributesImpl());"
101231,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java,624,,"          LOG.info(""Renaming reserved path "" + oldPath + "" to "" + newPath);"
101232,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageSerialization.java,335,,"  @SuppressWarnings(""deprecation"")"
101233,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java,1922,,"          "" to "" + target);"
101234,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java,2601,,"              src + "" for client "" + clientMachine);"
101235,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java,2823,,"            ""failed to create file "" + src + "" for "" + holder +"
101236,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java,7484,,"    innerInfo.put(""min"", StringUtils.format(""%.2f%%"", min));"
101237,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java,7508,,"          jasMap.put(""stream"", ""Failed"");"
101238,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java,204,,"      + StartupOption.BACKUP.getName() + ""] | \n\t["""
101239,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java,546,,"                         +src+"" for ""+clientName+"" at ""+clientMachine);"
101240,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java,550,,"          + MAX_PATH_LENGTH + "" characters, "" + MAX_PATH_DEPTH + "" levels."");"
101241,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java,550,,"          + MAX_PATH_LENGTH + "" characters, "" + MAX_PATH_DEPTH + "" levels."");"
101242,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java,837,,"            .append("" %)"");"
101243,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java,340,,"  @Consumes({""*/*""})"
101244,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java,402,,"  @Path(""{"" + UriFsPathParam.NAME + "":.*}"")"
101245,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java,511,,"    final NameNode namenode = (NameNode)context.getAttribute(""name.node"");"
101246,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java,526,,"      final String js = JsonUtil.toJsonString(""boolean"", b);"
101247,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java,631,,"      throw new UnsupportedOperationException(op + "" is not supported"");"
101248,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,135,,"      if (maxTtlString.equalsIgnoreCase(""never"")) {"
101249,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,199,,"      listing.addRow(""-force"","
101250,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,217,,"      String path = StringUtils.popOptionWithArgument(""-path"", args);"
101251,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,251,,"        System.err.println(""Can't understand argument: "" + args.get(0));"
101252,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,318,,"        System.err.println(""Usage is "" + getShortUsage());"
101253,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,514,,"      listing.addRow(""-stats"", ""List path-based cache directive statistics."");"
101254,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,689,,"        System.err.print(""Can't understand arguments: "" +"
101255,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/CacheAdmin.java,808,,"        prefix = "" and "";"
101256,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,483,,"      printUsage(""-safemode"");"
101257,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,717,,"      System.err.println(""Usage: java DFSAdmin"""
101258,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,789,,"      ""\t["" + SetQuotaCommand.USAGE + ""]\n"" +"
101259,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,789,,"      ""\t["" + SetQuotaCommand.USAGE + ""]\n"" +"
101260,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,1434,,"    } else if (""-deleteBlockPool"".equals(cmd)) {"
101261,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,1443,,"    } else if (""-shutdownDatanode"".equals(cmd)) {"
101262,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java,1461,,"      System.err.println(""           [""+RollingUpgradeCommand.USAGE+""]"");"
101263,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/XmlEditsVisitor.java,65,,"      throw new IOException(""SAX error: "" + e.getMessage());"
101264,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/PBImageXmlWriter.java,238,,"    o(""referredId"", r.getReferredId()).o(""name"", r.getName().toStringUtf8())"
101265,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/LightWeightHashSet.java,107,,"  @SuppressWarnings(""unchecked"")"
101266,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java,209,,"    m.put(""length"", status.getLen());"
101267,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java,210,,"    m.put(""owner"", status.getOwner());"
101268,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java,211,,"    m.put(""group"", status.getGroup());"
101269,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java,297,,"    m.put(""name"", datanodeinfo.getXferAddr());"
101270,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/TestFuseDFS.java,90,,"    LOG.debug(""EXEC ""+cmd);"
101271,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/main/native/fuse-dfs/test/TestFuseDFS.java,293,,"    execAssertSucceeds(""rm "" + f.getAbsolutePath());"
101272,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/server/datanode/TestFiDataTransferProtocol.java,99,,"    FiTestUtil.LOG.info(""Running "" + methodName + "" ..."");"
101273,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/server/datanode/TestFiDataTransferProtocol.java,99,,"    FiTestUtil.LOG.info(""Running "" + methodName + "" ..."");"
101274,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/server/datanode/TestFiDataTransferProtocol2.java,133,,"    FiTestUtil.LOG.info(""Running "" + methodName + "" ..."");"
101275,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/aop/org/apache/hadoop/hdfs/server/datanode/TestFiDataTransferProtocol2.java,133,,"    FiTestUtil.LOG.info(""Running "" + methodName + "" ..."");"
101276,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,77,,"    RefreshRegistry.defaultRegistry().register(""firstHandler"", firstHandler);"
101277,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,81,,"    Mockito.stub(secondHandler.handleRefresh(""secondHandler"", new String[]{""one"", ""two""}))"
101278,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,81,,"    Mockito.stub(secondHandler.handleRefresh(""secondHandler"", new String[]{""one"", ""two""}))"
101279,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,81,,"    Mockito.stub(secondHandler.handleRefresh(""secondHandler"", new String[]{""one"", ""two""}))"
101280,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,97,,"    String [] args = new String[]{""-refresh"", ""nn""};"
101281,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,105,,"    String [] args = new String[]{""-refresh"", ""localhost:"" + "
101282,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,163,,"    RefreshRegistry.defaultRegistry().register(""sharedId"", firstHandler);"
101283,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,192,,"    RefreshRegistry.defaultRegistry().register(""shared"", handlerOne);"
101284,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/TestGenericRefresh.java,220,,"    RefreshRegistry.defaultRegistry().register(""exceptional"", exceptionalHandler);"
101285,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestEnhancedByteBufferAccess.java,155,,"        Assert.fail(""unexpected InterruptedException during "" +"
101286,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestEnhancedByteBufferAccess.java,156,,"            ""waitReplication: "" + e);"
101287,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestEnhancedByteBufferAccess.java,158,,"        Assert.fail(""unexpected TimeoutException during "" +"
101288,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java,443,,"      String[] files = new String[] { USER_DIR + ""/a"", USER_DIR + ""/a/b"" };"
101289,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java,858,,"      wrap.mkdir(new Path(USER_DIR + ""/alpha""), FsPermission.getDirDefault(),"
101290,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java,861,,"          + ""/alphaLink""), false);"
101291,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java,868,,"      Assert.assertEquals(USER_DIR + ""/alpha/beta"", statuses[0].getPath()"
101292,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestGlobPaths.java,1002,,"      statuses = wrap.globStatus(new Path(USER_DIR + ""/alphaLinkz/betaz""),"
101293,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java,109,,"    Path link      = new Path(testBaseDir1(), ""linkToFile"");"
101294,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java,126,,"    Path hdfsFile    = new Path(testBaseDir1(), ""file"");"
101295,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java,201,,"    wrapper.setOwner(linkToFile, ""user"", ""group"");"
101296,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfs.java,201,,"    wrapper.setOwner(linkToFile, ""user"", ""group"");"
101297,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestSymlinkHdfsFileSystem.java,53,,"    Path link = new Path(testBaseDir1(), ""link"");"
101298,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/TestXAttr.java,38,,"      .setName(""name"")"
101299,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/permission/TestStickyBit.java,105,,"    Path file = new Path(p, ""foo"");"
101300,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/shell/TestHdfsTextCommand.java,93,,"      System.getProperty(""line.separator"") +"
101301,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFileSystemWithAcls.java,121,,"        aclEntry(ACCESS, USER, ""foo"", READ),"
101302,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/fs/viewfs/TestViewFsWithAcls.java,121,,"        aclEntry(ACCESS, USER, ""foo"", READ),"
101303,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java,550,,"          + "" Live = ""+live.size()+"" Expected = ""+expectedLive"
101304,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/DFSTestUtil.java,1143,,"    filesystem.addCachePool(new CachePoolInfo(""pool1""));"
101305,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java,1336,,"          + ""] is less than the number of datanodes ["" + numDataNodes + ""]."");"
101306,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java,2623,,"        conf.setIfUnset(DFS_DATANODE_ADDRESS_KEY, ""127.0.0.1:0"");"
101307,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSClusterWithNodeGroup.java,88,,"          + ""] is less than the number of datanodes ["" + numDataNodes + ""]."");"
101308,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestAppendDifferentChecksum.java,73,,"    FileSystem fsWithSmallChunk = createFsWithChecksum(""CRC32"", 512);"
101309,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockReaderFactory.java,117,,"    String TEST_FILE = ""/test_file"";"
101310,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestBlockReaderLocal.java,152,,"            ""waitReplication: "" + e);"
101311,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestCrcCorruption.java,163,,"      util.createFiles(fs, ""/srcdat"", replFactor);"
101312,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java,116,,"    Path withPort = new Path(""hdfs://"" +"
101313,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java,540,,"      LOG.info(""Test 1 succeeded! Time spent: "" + (timestamp2-timestamp)/1000.0 + "" sec."");"
101314,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java,71,,"      File curDir = new File(nameNodeDirs[i], ""current"");"
101315,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSFinalize.java,86,,"      assertFalse(new File(nameNodeDirs[i],""previous"").isDirectory());"
101316,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java,76,,"      File curDir = new File(baseDir, ""current"");"
101317,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSRollback.java,94,,"      assertFalse(new File(baseDirs[i],""previous"").isDirectory());"
101318,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,82,,"  private static final String USER_A1 = ""user.a1"";"
101319,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,134,,"    assertTrue(""Not a HDFS: ""+fs.getUri(),"
101320,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,222,,"      args[0] = ""-du"";"
101321,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,228,,"        System.err.println(""Exception raised from DFSShell.run "" +"
101322,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,352,,"      Path root = new Path(""/nonexistentfile"");"
101323,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,358,,"      argv[0] = ""-cat"";"
101324,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,366,,"      argv[0] = ""-rm"";"
101325,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,375,,"          (returned.lastIndexOf(""No such file or directory"") != -1));"
101326,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,398,,"      argv[0] = ""-ls"";"
101327,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,423,,"      argv[0] = ""-mkdir"";"
101328,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,430,,"      Path testFile = new Path(""/testfile"");"
101329,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,477,,"      argv[0] = ""-test"";"
101330,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,534,,"      argv[2] = dstFs.getUri().toString() + ""/furi"";"
101331,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,538,,"      argv[0] = ""-cp"";"
101332,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,558,,"      runCmd(shell, ""-chgrp"", ""-R"", ""herbivores"", dstFs.getUri().toString() +""/*"");"
101333,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,558,,"      runCmd(shell, ""-chgrp"", ""-R"", ""herbivores"", dstFs.getUri().toString() +""/*"");"
101334,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,560,,"      runCmd(shell, ""-chown"", ""-R"", "":reptiles"", dstFs.getUri().toString() + ""/"");"
101335,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,614,,"          fs.create(new Path(root, ""file.gz"")));"
101336,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,629,,"      argv[0] = ""-text"";"
101337,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,632,,"      assertEquals(""'-text "" + argv[1] + "" returned "" + ret, 0, ret);"
101338,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,632,,"      assertEquals(""'-text "" + argv[1] + "" returned "" + ret, 0, ret);"
101339,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,633,,"      assertTrue(""Output doesn't match input"","
101340,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,659,,"      byte[] outbytes = ""foo"".getBytes();"
101341,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,696,,"      writebytes = ""bar"".getBytes();"
101342,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,744,,"        assertTrue(""Copying failed."", f1.isFile());"
101343,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,749,,"        File sub = new File(localroot, ""sub"");"
101344,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,879,,"      LOG.info(""RUN: ""+args[0]+"" exit="" + exitCode);"
101345,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,918,,"      runCmd(shell, ""-chmod"", ""-R"", ""a+rwX"", chmodDir);"
101346,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,1440,,"      UserGroupInformation.createUserForTesting(""tmpname"", new String[] {""mygroup""});"
101347,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,1447,,"      Path p = new Path(""/foo"");"
101348,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,1466,,"                     str.indexOf(""Permission denied"") != -1);"
101349,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,1657,,"      Path src = new Path(hdfsTestDir, ""srcfile"");"
101350,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,1801,,"    final Path rawHdfsTestDir = new Path(""/.reserved/raw"" + testdir);"
101351,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2469,,"              ""-setfattr"", ""-n"", ""user.a1"", ""-v"", ""1234"", ""/foo""});"
101352,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2469,,"              ""-setfattr"", ""-n"", ""user.a1"", ""-v"", ""1234"", ""/foo""});"
101353,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2470,,"          assertEquals(""Returned should be 1"", 1, ret);"
101354,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2472,,"          assertTrue(""Permission denied printed"", "
101355,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2481,,"      assertEquals(""Returned should be 0"", 0, ret);"
101356,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2491,,"              ""-getfattr"", ""-n"", ""user.a1"", ""/foo""});"
101357,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2531,,"      Path p = new Path(""/mydir"");"
101358,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2542,,"        new String[] {""user.Foo""},"
101359,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java,2546,,"        new String[] {""-setfattr"", ""-n"", ""user.FOO"", ""/mydir""},"
101360,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java,143,,"      UpgradeUtilities.createNameNodeStorageDirs(baseDirs, ""current"");"
101361,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStorageStateRecovery.java,145,,"      UpgradeUtilities.createNameNodeStorageDirs(baseDirs, ""previous"");"
101362,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUpgrade.java,90,,"          Joiner.on(""  \n"").join(new File(baseDir, ""current"").list()));"
101363,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUpgrade.java,95,,"      assertExists(new File(baseDir,""current/"" "
101364,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUpgradeFromImage.java,89,,"    if (System.getProperty(""test.build.data"") == null) { // to allow test to be run outside of Maven"
101365,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,146,,"    conf.set(DFS_NAMESERVICES, ""nn1"");"
101366,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,203,,"    conf.set(DFS_NAMESERVICES, ""nn1,nn2"");"
101367,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,206,,"    conf.set(DFSUtil.addKeySuffixes(DFS_NAMENODE_RPC_ADDRESS_KEY, ""nn2""),"
101368,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,233,,"    conf.set(DFS_NAMESERVICES, ""ns1"");"
101369,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,398,,"    NameNode.initializeGenericKeys(newConf, ""ns2"", ""nn1"");"
101370,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,477,,"    final String NS1_NN1_HOST = ""ns1-nn1.example.com:8020"";"
101371,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,481,,"    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, ""hdfs://ns1"");"
101372,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,490,,"          DFS_NAMENODE_RPC_ADDRESS_KEY, ""ns1"", ""ns1-nn1""),"
101373,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSUtil.java,634,,"    conf.set(DFS_NAMENODE_RPC_ADDRESS_KEY, ""hdfs://"" + NN1_ADDR);"
101374,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDatanodeBlockScanner.java,470,,"        Path fileName = new Path(""/test"" + i);"
101375,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java,396,,"    assertEquals(""All datanodes must be alive"", numDatanodes,"
101376,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDecommission.java,597,,"            + tries + "" times."", tries < 20);"
101377,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDefaultNameNodePort.java,35,,"    assertEquals(NameNode.getAddress(""foo"").getPort(),"
101378,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java,351,,"    final Path superPathFile = new Path(superPath, ""file1"");"
101379,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java,388,,"        assertEquals(""expected ez path"", allPath.toString(),"
101380,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFSInputChecker.java,94,,"    checkAndEraseData(actual, 0, expected, ""Read Sanity Test"");"
101381,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileAppend4.java,313,,"      Path f = new Path(""/testAppend"");"
101382,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java,491,,"      System.out.println(""locations = "" + locations.locatedBlockCount());"
101383,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java,526,,"      System.out.println(""testFileCreationError2: """
101384,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java,625,,"      System.out.println(""testFileCreationNamenodeRestart: """
101385,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCreation.java,626,,"                         + ""Created file "" + file1);"
101386,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFsShellPermission.java,138,,"      doAsGroup = doAsUser.equals(""hdfs"")? ""supergroup"" : ""users"";"
101387,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFsShellPermission.java,171,,"        new FileEntry(""userA"", true, ""userA"", ""users"", ""755""),"
101388,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFsShellPermission.java,172,,"        new FileEntry(""userA/userB"", true, ""userB"", ""users"", targetPerm)"
101389,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFsShellPermission.java,181,,"    return genDeleteEmptyDirHelper(""-rm -r"", ""744"", ""userA"", true);"
101390,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFsShellPermission.java,186,,"    return genDeleteEmptyDirHelper(""-rm -r"", ""700"", ""userA"", true);"
101391,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestHDFSServerPorts.java,102,,"    FileSystem.setDefaultUri(config, ""hdfs://"" + THIS_HOST);"
101392,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java,189,,"      Assert.assertTrue(pRenamed+"" not found"", fs2.exists(pRenamed));"
101393,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java,190,,"      Assert.assertFalse(""has lease for ""+p, hasLease(cluster, p));"
101394,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLease.java,191,,"      Assert.assertTrue(""no lease for ""+pRenamed, hasLease(cluster, pRenamed));"
101395,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java,168,,"    AppendTestUtil.LOG.info(""filestr="" + filestr);"
101396,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java,174,,"    AppendTestUtil.LOG.info(""size="" + size);"
101397,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java,178,,"    AppendTestUtil.LOG.info(""hflush"");"
101398,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestLeaseRecovery2.java,182,,"      AppendTestUtil.LOG.info(""leasechecker.interruptAndJoin()"");"
101399,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPeerCache.java,156,,"    DatanodeID dnId = new DatanodeID(""192.168.0.1"","
101400,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestPeerCache.java,157,,"          ""fakehostname"", ""fake_datanode_id"","
101401,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java,107,,"      String[] args = new String[]{""-setQuota"", ""3"", parent.toString()};"
101402,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java,111,,"      runCommand(admin, false, ""-setSpaceQuota"", ""2t"", parent.toString());"
101403,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java,164,,"      runCommand(admin, new String[]{""-clrQuota"", parent.toString()}, false);"
101404,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java,190,,"      runCommand(admin, false, ""-clrSpaceQuota"", parent.toString());"
101405,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestQuota.java,400,,"      Path tempPath = new Path(quotaDir3, ""nqdir32"");"
101406,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java,60,,"    conf.setInt(""ipc.client.connection.maxidletime"", MAX_IDLE_TIME);"
101407,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java,86,,"      Path file1 = new Path(dir1, ""file1"");"
101408,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java,88,,"      System.out.println(""testFileCreationDeleteParent: """
101409,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java,89,,"          + ""Created file "" + file1);"
101410,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRenameWhileOpen.java,94,,"      Path dir2 = new Path(""/user/dir2"");"
101411,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java,153,,"    dfsClient = new DFSClient(new InetSocketAddress(""localhost"","
101412,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReplication.java,476,,"      FSDataOutputStream create = fs.create(new Path(""/test""));"
101413,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java,124,,"    final Path baseFileRaw = new Path(zone, ""/.reserved/raw/base"");"
101414,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java,158,,"    final Path reservedRaw = new Path(""/.reserved/raw"");"
101415,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java,241,,"          fail(""access to /.reserved/raw is superuser-only operation"");"
101416,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestReservedRawPaths.java,243,,"          assertExceptionContains(""Superuser privilege is required"", e);"
101417,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRestartDFS.java,60,,"      fs.setOwner(rootpath, rootstatus.getOwner() + ""_XXX"", null);"
101418,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java,72,,"      final Path foo = new Path(""/foo"");"
101419,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java,73,,"      final Path bar = new Path(""/bar"");"
101420,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java,82,,"        runCmd(dfsadmin, false, ""-rollingUpgrade"", ""abc"");"
101421,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgradeRollback.java,107,,"          dfsadmin.run(new String[] { ""-rollingUpgrade"", ""prepare"" }));"
101422,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSetTimes.java,135,,"      System.out.println(""atime on "" + file1 + "" is "" + adate + "
101423,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestSnapshotCommands.java,73,,"    fs.mkdirs(new Path(""/sub1""));"
101424,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java,161,,"        new File(namenodeStorage, ""current""), false);"
101425,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/UpgradeUtilities.java,309,,"          (list[i].getName().equals(""VERSION"") ||"
101426,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/TestSaslDataTransfer.java,63,,"      ""authentication,integrity,privacy"");"
101427,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/protocolPB/TestPBHelper.java,446,,"        DFSTestUtil.getLocalDatanodeInfo(""127.0.0.1"", ""h1"","
101428,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/TestNNWithQJM.java,70,,"        mjc.getQuorumJournalURI(""myjournal"").toString());"
101429,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumCall.java,45,,"    q.waitFor(1, 0, 0, 100000, ""test""); // wait for 1 response"
101430,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java,326,,"        GenericTestUtils.assertExceptionContains(""mock failure"", qe);"
101431,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java,341,,"        ""edits_.*"","
101432,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java,463,,"    futureThrows(new IOException(""injected"")).when(spies.get(0))"
101433,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManager.java,654,,"    File paxos0 = new File(cluster.getCurrentDir(0, JID), ""paxos"");"
101434,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/client/TestQuorumJournalManagerUnit.java,131,,"    futureThrows(new IOException(""logger failed""))"
101435,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/TestDelegationToken.java,100,,"        ""SomeUser"", ""JobTracker"");"
101436,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/security/token/block/TestBlockToken.java,165,,"        blockKeyUpdateInterval, blockTokenLifetime, 0, ""fake-pool"", null);"
101437,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java,593,,"      args.add(""-exclude"");"
101438,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java,610,,"      args.add(""-include"");"
101439,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java,732,,"    String parameters[] = new String[] { ""-threshold"", ""0"" };"
101440,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java,924,,"    excludeHosts.add( ""datanodeY"");"
101441,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java,925,,"    excludeHosts.add( ""datanodeZ"");"
101442,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java,927,,"        new HostNameBasedNodes(new String[] {""datanodeX"", ""datanodeY"", ""datanodeZ""},"
101443,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java,89,,"        ""/rackA"","
101444,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java,141,,"    assertTrue(""Source of replication should be one of the nodes the block "" +"
101445,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java,142,,"        ""was on. Was: "" + pipeline[0],"
101446,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlocksWithNotEnoughRacks.java,89,,"    final Path filePath = new Path(""/testFile"");"
101447,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlocksWithNotEnoughRacks.java,91,,"    String racks[] = {""/rack1"", ""/rack1"", ""/rack1""};"
101448,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlocksWithNotEnoughRacks.java,103,,"      String newRacks[] = {""/rack2""};"
101449,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestCorruptReplicaInfo.java,94,,"      assertEquals(""Number of corrupt blocks not returning correctly"","
101450,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHost2NodesMap.java,37,,"        DFSTestUtil.getDatanodeDescriptor(""1.1.1.1"", ""/d1/r1""),"
101451,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHost2NodesMap.java,38,,"        DFSTestUtil.getDatanodeDescriptor(""2.2.2.2"", ""/d1/r1""),"
101452,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHost2NodesMap.java,39,,"        DFSTestUtil.getDatanodeDescriptor(""3.3.3.3"", ""/d1/r2""),"
101453,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHostFileManager.java,45,,"    s.add(entry(""127.0.0.1:12345""));"
101454,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHostFileManager.java,54,,"    s.add(entry(""127.0.0.1""));"
101455,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHostFileManager.java,63,,"    s.add(entry(""127.0.0.1:123""));"
101456,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHostFileManager.java,65,,"    Assert.assertFalse(s.match(entry(""127.0.0.1:12"")));"
101457,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHostFileManager.java,70,,"    Assert.assertFalse(s.match(entry(""127.0.0.2"")));"
101458,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestHostFileManager.java,71,,"    Assert.assertFalse(s.match(entry(""127.0.0.2:123"")));"
101459,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java,108,,"        ""/d1/r1"","
101460,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java,110,,"        ""/d1/r2"","
101461,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicy.java,112,,"        ""/d2/r3"","
101462,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicyWithNodeGroup.java,67,,"        ""/d1/r1/n1"","
101463,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicyWithNodeGroup.java,69,,"        ""/d1/r1/n2"","
101464,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestReplicationPolicyWithNodeGroup.java,70,,"        ""/d1/r2/n3"","
101465,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java,87,,"    conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, ""hdfs://localhost:4321/"");"
101466,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java,90,,"    String user = ""TheDoctor"";"
101467,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java,105,,"    conf.set(DFSConfigKeys.HADOOP_SECURITY_AUTHENTICATION, ""kerberos"");"
101468,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java,202,,"      Assert.fail(""bad request allowed"");"
101469,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java,241,,"          ""Security enabled but user not authenticated by filter"","
101470,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java,190,,"    Path filePath = new Path(""/"" + METHOD_NAME + "".dat"");"
101471,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java,338,,"    assertThat(""Wrong number of corrupt blocks"","
101472,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java,340,,"    assertThat(""Wrong number of PendingDeletion blocks"","
101473,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/BlockReportTestBase.java,444,,"    assertThat(""Wrong number of PendingReplication blocks"","
101474,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java,685,,"      throw new ReplicaNotFoundException(""Block "" + b"
101475,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockRecovery.java,237,,"      LOG.debug(""Running "" + GenericTestUtils.getMethodName());"
101476,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java,217,,"    String TEST_PATH = ""/test"";"
101477,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java,92,,"      FileUtil.setExecutable(new File(dataDir, ""data""+(2*i+1)), true);"
101478,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java,135,,"    assertTrue(""Couldn't chmod local vol"", FileUtil.setExecutable(dn1Vol1, false));"
101479,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureReporting.java,154,,"    assertCounter(""VolumeFailures"", 1L, "
101480,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java,93,,"    assumeTrue(!System.getProperty(""os.name"").startsWith(""Windows""));"
101481,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeVolumeFailureToleration.java,93,,"    assumeTrue(!System.getProperty(""os.name"").startsWith(""Windows""));"
101482,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java,259,,"      long cmds = MetricsAsserts.getLongCounter(""BlocksCached"", dnMetrics);"
101483,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestFsDatasetCache.java,481,,"    dfs.addCachePool(new CachePoolInfo(""pool""));"
101484,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestSimulatedFSDataset.java,113,,"      assertTrue(""Expected an IO exception"", false);"
101485,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/TestAvailableSpaceVolumeChoosingPolicy.java,58,,"    @SuppressWarnings(""unchecked"")"
101486,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,62,,"    UserGroupInformation.createUserForTesting(""bruce"", new String[] { });"
101487,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,64,,"    UserGroupInformation.createUserForTesting(""diana"", new String[] { });"
101488,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,104,,"      aclEntry(ACCESS, USER, ""foo"", ALL),"
101489,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,302,,"      aclEntry(ACCESS, USER, ""bar"", READ_WRITE),"
101490,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,842,,"    Path filePath = new Path(path, ""file1"");"
101491,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,891,,"    Path dirPath = new Path(path, ""dir1"");"
101492,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSAclBaseTest.java,1127,,"    Path bruceFile = new Path(bruceDir, ""file"");"
101493,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,372,,"      Assert.fail(""expected IOException"");"
101494,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,413,,"        createUserForTesting(""user"", new String[] {""mygroup""});"
101495,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,413,,"        createUserForTesting(""user"", new String[] {""mygroup""});"
101496,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,414,,"    fs.setXAttr(path, ""trusted.foo"", ""1234"".getBytes());"
101497,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,414,,"    fs.setXAttr(path, ""trusted.foo"", ""1234"".getBytes());"
101498,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,449,,"      GenericTestUtils.assertExceptionContains(""Permission denied"", e);"
101499,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSXAttrBaseTest.java,1032,,"            fail(""getXAttr should have thrown"");"
101500,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java,331,,"      if(args.size() < 2 || ! args.get(0).startsWith(""-op""))"
101501,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java,371,,"      LOG.info(""--- "" + getOpName() + "" stats  ---"");"
101502,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java,494,,"      LOG.info(""--- "" + getOpName() + "" inputs ---"");"
101503,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java,713,,"      ""-op "" + OP_OPEN_NAME + OP_USAGE_ARGS;"
101504,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java,1370,,"        + "" | \n\t"" + CreateFileStats.OP_CREATE_USAGE"
101505,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclConfigFlag.java,70,,"      aclEntry(DEFAULT, USER, ""foo"", READ_WRITE)));"
101506,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclTransformation.java,59,,"      .add(aclEntry(ACCESS, USER, ""bruce"", READ_WRITE))"
101507,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclTransformation.java,60,,"      .add(aclEntry(ACCESS, USER, ""diana"", READ_EXECUTE))"
101508,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclTransformation.java,62,,"      .add(aclEntry(ACCESS, GROUP, ""sales"", READ_EXECUTE))"
101509,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAclTransformation.java,92,,"      aclEntry(ACCESS, USER, ""clark""),"
101510,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddBlockRetry.java,113,,"          nn.addBlock(src, ""clientName"", null, null,"
101511,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAuditLogs.java,177,,"    fs.setOwner(file, ""root"", null);"
101512,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,168,,"    final String poolName = ""pool1"";"
101513,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,187,,"      GenericTestUtils.assertExceptionContains(""invalid empty cache pool name"","
101514,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,194,,"      GenericTestUtils.assertExceptionContains(""CachePoolInfo is null"", ioe);"
101515,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,254,,"      fail(""expected to get an exception when "" +"
101516,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,255,,"          ""removing a non-existent pool."");"
101517,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,297,,"    info = new CachePoolInfo(""pool2"");"
101518,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,865,,"    final String pool = ""friendlyPool"";"
101519,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,930,,"    paths.add(new Path(""/foo/bar""));"
101520,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,934,,"    dfs.mkdir(new Path(""/foo""), FsPermission.getDirDefault());"
101521,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,1291,,"      assertExceptionContains(""exceeds the max relative expiration"", e);"
101522,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCacheDirectives.java,1308,,"      fail(""Modified a directive to exceed pool's max relative expiration"");"
101523,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java,249,,"      fos = fs.create(new Path(""tmpfile0""));"
101524,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java,262,,"        fail(""Fault injection failed."");"
101525,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java,995,,"    conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, ""0.0.0.0:0"");"
101526,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java,1530,,"      Mockito.doThrow(new IOException(""Injecting failure before edit rename""))"
101527,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCheckpoint.java,2377,,"    opts.parse(""-checkpoint"");"
101528,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,109,,"    assertTrue(""Didn't get new ClusterId"", (cid != null && !cid.equals("""")) );"
101529,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,133,,"    String[] argv = { ""-format"" };"
101530,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,136,,"      fail(""createNameNode() did not call System.exit()"");"
101531,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,138,,"      assertEquals(""Format should have succeeded"", 0, e.status);"
101532,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,155,,"      fail(""Failed to create dir "" + hdfsDir.getPath());"
101533,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,183,,"    String[] argv = { ""-format"", ""-force"" };"
101534,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,209,,"    String[] argv = { ""-format"", ""-force"", ""-clusterid"", myId };"
101535,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,243,,"    File version = new File(hdfsDir, ""current/VERSION"");"
101536,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestClusterId.java,244,,"    assertFalse(""Check version should not exist"", version.exists());"
101537,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLog.java,686,,"        File currentDir = new File(nameDir, ""current"");"
101538,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogJournalFailures.java,108,,"      assertTrue(re.getClassName().contains(""ExitException""));"
101539,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogRace.java,279,,"        LOG.info(""Save "" + i + "": entering safe mode"");"
101540,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestEditLogRace.java,374,,"                new PermissionStatus(""test"",""test"", new FsPermission((short)00755)),"
101541,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageStorageInspector.java,47,,"        ""/foo/current/"" + getImageFileName(123),"
101542,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSImageWithAcl.java,67,,"    AclEntry e = new AclEntry.Builder().setName(""foo"")"
101543,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,57,,"    UserGroupInformation.createUserForTesting(""bruce"", new String[] { });"
101544,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,59,,"    UserGroupInformation.createUserForTesting(""diana"", new String[] { ""sales"" });"
101545,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,59,,"    UserGroupInformation.createUserForTesting(""diana"", new String[] { ""sales"" });"
101546,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,61,,"    UserGroupInformation.createUserForTesting(""clark"", new String[] { ""execs"" });"
101547,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,84,,"    INodeFile inodeFile = createINodeFile(inodeRoot, ""file1"", ""bruce"", ""execs"","
101548,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,91,,"    assertPermissionGranted(BRUCE, ""/file1"", READ);"
101549,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,136,,"    INodeDirectory inodeDir = createINodeDirectory(inodeRoot, ""dir1"", ""bruce"","
101550,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSPermissionChecker.java,146,,"    assertPermissionGranted(BRUCE, ""/dir1/file1"", READ_WRITE);"
101551,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java,75,,"    mkdirs(""/22"", null);"
101552,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java,76,,"    mkdirs(""/333"", null);"
101553,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsLimits.java,77,,"    mkdirs(""/4444"", null);"
101554,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java,138,,"    DFSTestUtil util = new DFSTestUtil.Builder().setName(""TestFsck"")."
101555,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java,149,,"      final String fileName = ""/srcdat"";"
101556,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFsck.java,742,,"      util.createFiles(fs, ""/corruptData"", (short) 1);"
101557,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGenericJournalConf.java,36,,"  private static final String DUMMY_URI = ""dummy://test"";"
101558,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestGetImageServlet.java,47,,"    conf.set(DFSConfigKeys.DFS_NAMESERVICES, ""ns1"");"
101559,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java,860,,"    testPath = ""/.reserved/.inodes/"" + INodeId.ROOT_INODE_ID;"
101560,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java,1042,,"      hdfs.mkdirs(new Path(""/tmp""));"
101561,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestLeaseManager.java,39,,"    lm.addLease(""holder1"", ""/a/b"");"
101562,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestLeaseManager.java,40,,"    lm.addLease(""holder2"", ""/a/c"");"
101563,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,81,,"      util.createFiles(fs, ""/srcdat10"");"
101564,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,87,,"      assertTrue(""Namenode has "" + badFiles.size()"
101565,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,122,,"      LOG.info(""Namenode has bad files. "" + badFiles.size());"
101566,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,275,,"      util.createFiles(fs, ""/corruptData"");"
101567,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,296,,"            LOG.info(""Deliberately removing file "" + blockFile.getName());"
101568,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,297,,"            assertTrue(""Cannot remove file."", blockFile.delete());"
101569,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,344,,"      util.createFiles(fs, ""/goodData"");"
101570,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestListCorruptFileBlocks.java,460,,"      util.createFiles(fs, ""/srcdat2"", (short) 1);"
101571,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionFunctional.java,85,,"      assertGlobEquals(cd0, ""fsimage_\\d*"", "
101572,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionFunctional.java,89,,"      assertGlobEquals(cd0, ""edits_.*"","
101573,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionManager.java,73,,"    tc.addRoot(""/foo1"", NameNodeDirType.IMAGE_AND_EDITS);"
101574,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionManager.java,74,,"    tc.addImage(""/foo1/current/"" + getImageFileName(100), true);"
101575,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionManager.java,95,,"    tc.addRoot(""/foo2"", NameNodeDirType.IMAGE_AND_EDITS);"
101576,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNNStorageRetentionManager.java,98,,"    tc.addImage(""/foo2/current/"" + getImageFileName(200), true);"
101577,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameEditsConfigs.java,86,,"      = FileJournalManager.matchEditLogs(new File(dir, ""current"").listFiles()); "
101578,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameEditsConfigs.java,441,,"      assertTrue(new File(nameAndEditsDir, ""current/VERSION"").exists());"
101579,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameEditsConfigs.java,605,,"      assertTrue(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY + "" must be trimmed "","
101580,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeMXBean.java,197,,"        assertTrue(statusMap.get(""active"").containsKey(nameDir.getAbsolutePath()));"
101581,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeOptionParsing.java,35,,"    opt = NameNode.parseArguments(new String[] {""-upgrade""});"
101582,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeOptionParsing.java,41,,"        ""mycid"" });"
101583,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeOptionParsing.java,47,,"        ""mycid"", ""-renameReserved"","
101584,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRecovery.java,492,,"    conf.set(DFSConfigKeys.DFS_NAMESERVICES, ""ns1"");"
101585,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRespectsBindHostKeys.java,98,,"      assertThat(""Bind address "" + address + "" is not wildcard."","
101586,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java,188,,"    namesystem.createSymlink(target, ""/a/b"", perm, true);"
101587,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java,211,,"    HdfsFileStatus status = namesystem.startFile(src, perm, ""holder"","
101588,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java,212,,"        ""clientmachine"", EnumSet.of(CreateFlag.CREATE), true, (short) 1, "
101589,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java,244,,"    LocatedBlock b = namesystem.appendFile(src, ""holder"", ""clientMachine"");"
101590,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java,353,,"    String name = namesystem.createSnapshot(dir, ""snap1"");"
101591,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNamenodeRetryCache.java,369,,"    namesystem.renameSnapshot(dir, ""snap1"", ""snap2"");"
101592,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestParallelImageWrite.java,81,,"      fs.setOwner(rootpath, rootstatus.getOwner() + ""_XXX"", null);"
101593,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestProcessCorruptBlocks.java,65,,"      final Path fileName = new Path(""/foo1"");"
101594,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartup.java,98,,"        fileAsURI(new File(hdfsDir, ""name"")).toString());"
101595,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartup.java,274,,"        LOG.info(""--image file "" + imf.getAbsolutePath() + ""; len = "" + imf.length() + ""; expected = "" + expectedImgSize);"
101596,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartup.java,302,,"        fileAsURI(new File(hdfsDir, ""chkpt"")).toString());"
101597,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartup.java,423,,"    namenode.getNamesystem().mkdirs(""/test"","
101598,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupOptionUpgrade.java,120,,"    storage.setClusterID(""currentcid"");"
101599,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,72,,"      .put(""percentComplete"", 0.0f)"
101600,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,75,,"          .put(""name"", ""LoadingFsImage"")"
101601,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,76,,"          .put(""desc"", ""Loading fsimage"")"
101602,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,77,,"          .put(""status"", ""PENDING"")"
101603,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,77,,"          .put(""status"", ""PENDING"")"
101604,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,79,,"          .put(""steps"", Collections.emptyList())"
101605,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,119,,"          .put(""status"", ""COMPLETE"")"
101606,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,125,,"              .put(""count"", 100L)"
101607,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,126,,"              .put(""total"", 100L)"
101608,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStartupProgressServlet.java,139,,"              .put(""file"", ""file"")"
101609,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStorageRestore.java,186,,"        new File(path1, ""current/"" + getInProgressEditsFileName(1)),"
101610,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestStorageRestore.java,422,,"      assertTrue(FileUtil.chmod(nameDir2, ""755"") == 0);"
101611,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestXAttrConfigFlag.java,62,,"    fs.setXAttr(PATH, ""user.foo"", null);"
101612,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/HATestUtil.java,151,,"    FileSystem fs = FileSystem.get(new URI(""hdfs://"" + logicalName), conf);"
101613,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestBootstrapStandby.java,130,,"        new String[]{""-force""},"
101614,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java,168,,"      assertTrue(fs.mkdirs(new Path(""/foo1"")));"
101615,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java,182,,"      assertTrue(fs.mkdirs(new Path(""/foo2"")));"
101616,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java,190,,"      assertTrue(fs.mkdirs(new Path(""/foo3"")));"
101617,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDFSUpgradeWithHA.java,194,,"          new String[]{""-force""},"
101618,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestDelegationTokensWithHA.java,135,,"        getDelegationToken(fs, ""JobTracker"");"
101619,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestEditLogTailer.java,79,,"            new PermissionStatus(""test"",""test"", new FsPermission((short)00755)),"
101620,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAConfiguration.java,80,,"    Configuration conf = getHAConf(""ns1"", ""1.2.3.1"", ""1.2.3.2"");"
101621,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHAMetrics.java,59,,"      assertEquals(nn0.getHAState(), ""standby"");"
101622,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,127,,"    final Path test = new Path(""/test"");"
101623,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,197,,"    assertTrue(""Bad safemode status: '"" + status + ""'"", status"
101624,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,213,,"    banner(""Starting with NN0 active and NN1 standby, creating some blocks"");"
101625,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,220,,"    DFSTestUtil.createFile(fs, new Path(""/test2""), 5 * BLOCK_SIZE, (short) 3,"
101626,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,224,,"    banner(""Restarting standby"");"
101627,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,274,,"    banner(""Waiting for standby to catch up to active namespace"");"
101628,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java,339,,"    banner(""Removing the blocks without rolling the edit log"");"
101629,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPipelinesFailover.java,154,,"      LOG.info(""Starting with NN 0 active"");"
101630,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestPipelinesFailover.java,165,,"      LOG.info(""Failing over to NN 1"");"
101631,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java,1033,,"      client.setXAttr(src, ""user.key"", ""value"".getBytes(),"
101632,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java,1123,,"    AtMostOnceOp op = new CreateOp(client, ""/testfile"");"
101633,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestRetryCacheWithHA.java,1181,,"            setPool(""pool"")."
101634,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java,375,,"    assertCounter(""GetBlockLocations"", 0L, getMetrics(NN_METRICS));"
101635,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java,401,,"    long lastCkptTime = MetricsAsserts.getLongGauge(""LastCheckpointTime"","
101636,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java,405,,"    assertGauge(""LastWrittenTransactionId"", 1L, getMetrics(NS_METRICS));"
101637,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java,406,,"    assertGauge(""TransactionsSinceLastCheckpoint"", 1L, getMetrics(NS_METRICS));"
101638,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/metrics/TestNameNodeMetrics.java,407,,"    assertGauge(""TransactionsSinceLastLogRoll"", 1L, getMetrics(NS_METRICS));"
101639,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java,63,,"    UserGroupInformation.createUserForTesting(""bruce"", new String[] { });"
101640,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java,65,,"    UserGroupInformation.createUserForTesting(""diana"", new String[] { });"
101641,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java,175,,"    Path filePath = new Path(path, ""file1"");"
101642,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestAclWithSnapshot.java,176,,"    Path subdirPath = new Path(path, ""subdir1"");"
101643,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestNestedSnapshots.java,227,,"      SnapshotTestHelper.LOG.info(""The exception is expected."", ioe);"
101644,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java,65,,"    Path path = new Path(""/test"");"
101645,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestOpenFilesWithSnapshot.java,80,,"    fs.delete(new Path(""/test/test""), true);"
101646,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,99,,"  static private final Path file1 = new Path(sub1, ""file1"");"
101647,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,100,,"  static private final Path file2 = new Path(sub1, ""file2"");"
101648,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,134,,"    final Path foo = new Path(abc, ""foo"");"
101649,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,141,,"          + "" is snapshottable and already has snapshots"");"
101650,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,150,,"    final Path bar = new Path(xyz, ""bar"");"
101651,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,218,,"    System.out.println(""DiffList is "" + diffReport.toString());"
101652,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,330,,"    final Path sdir1 = new Path(""/dir1"");"
101653,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,331,,"    final Path sdir2 = new Path(""/dir2"");"
101654,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,336,,"    final Path bar2 = new Path(foo, ""bar2"");"
101655,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,354,,"        ""foo/bar"");"
101656,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,364,,"        ""foo/bar2"");"
101657,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,447,,"        ""foo/bar3"");"
101658,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,637,,"    final Path sdir3 = new Path(""/dir3"");"
101659,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,643,,"    final Path bar1_dir1 = new Path(foo_dir1, ""bar1"");"
101660,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,668,,"        ""foo/bar1"");"
101661,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,856,,"    SnapshotTestHelper.createSnapshot(hdfs, sdir2, ""s22"");"
101662,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,876,,"    SnapshotTestHelper.createSnapshot(hdfs, sdir3, ""s333"");"
101663,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,929,,"    SnapshotTestHelper.createSnapshot(hdfs, sdir2, ""s2222"");"
101664,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,1269,,"    final Path dir2file = new Path(sdir2, ""file"");"
101665,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,1416,,"    final Path foo_dir2 = new Path(sdir2, ""foo2"");"
101666,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,1547,,"    final Path test = new Path(""/test"");"
101667,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,1548,,"    final Path dir1 = new Path(test, ""dir1"");"
101668,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java,1549,,"    final Path dir2 = new Path(test, ""dir2"");"
101669,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java,310,,"    Path sub = new Path(dir, ""sub"");"
101670,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshot.java,364,,"    Path file0 = new Path(dir, ""file0"");"
101671,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java,223,,"    Path foo = new Path(""/foo"");"
101672,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotBlocksMap.java,256,,"    final Path bar = new Path(foo, ""bar"");"
101673,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java,115,,"    Path file0 = new Path(sub, ""file0"");"
101674,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java,116,,"    Path file1 = new Path(sub, ""file1"");"
101675,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java,272,,"    Path newFileAfterS0 = new Path(subsub, ""newFile"");"
101676,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java,494,,"    hdfs.setOwner(metaChangeDir, ""unknown"", ""unknown"");"
101677,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java,545,,"      fail(""should throw FileNotFoundException"");"
101678,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDeletion.java,547,,"      GenericTestUtils.assertExceptionContains(""File does not exist: """
101679,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,93,,"    Path file11 = new Path(modifyDir, ""file11"");"
101680,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,96,,"    Path link13 = new Path(modifyDir, ""link13"");"
101681,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,248,,"            DFSUtil.string2Bytes(""subsub1/subsubsub1"")),"
101682,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,252,,"            DFSUtil.string2Bytes(""subsub1/subsubsub1/file11"")),"
101683,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,254,,"            DFSUtil.string2Bytes(""subsub1/subsubsub1/file13"")),"
101684,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,256,,"            DFSUtil.string2Bytes(""subsub1/subsubsub1/link13"")),"
101685,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,258,,"            DFSUtil.string2Bytes(""subsub1/subsubsub1/file15"")));"
101686,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,340,,"    final Path sdir1 = new Path(root, ""dir1"");"
101687,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,341,,"    final Path sdir2 = new Path(root, ""dir2"");"
101688,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,342,,"    final Path foo = new Path(sdir1, ""foo"");"
101689,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,343,,"    final Path bar = new Path(foo, ""bar"");"
101690,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,365,,"        new DiffReportEntry(DiffType.RENAME, DFSUtil.string2Bytes(""dir1/foo""),"
101691,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotDiffReport.java,385,,"    final Path fileInFoo = new Path(foo, ""file"");"
101692,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java,83,,"    assertGauge(""SnapshottableDirectories"", 0, getMetrics(NS_METRICS));"
101693,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java,84,,"    assertCounter(""AllowSnapshotOps"", 0L, getMetrics(NN_METRICS));"
101694,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java,137,,"    assertGauge(""Snapshots"", 0, getMetrics(NS_METRICS));"
101695,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotMetrics.java,138,,"    assertCounter(""CreateSnapshotOps"", 0L, getMetrics(NN_METRICS));"
101696,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestSnapshotRename.java,125,,"    hdfs.renameSnapshot(sub1, ""s3"", ""s22"");"
101697,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestXAttrWithSnapshot.java,351,,"    Path filePath = new Path(path, ""file1"");"
101698,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/TestStartupProgress.java,64,,"    Step loadingEditsFile = new Step(""file"", 1000L);"
101699,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/TestStartupProgress.java,153,,"    startupProgress.setFile(LOADING_FSIMAGE, ""file1"");"
101700,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/startupprogress/TestStartupProgress.java,174,,"    startupProgress.setFile(LOADING_FSIMAGE, ""file2"");"
101701,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java,133,,"        ExtendedBlockId key = new ExtendedBlockId(blockId, ""test_bp1"");"
101702,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitLocalRead.java,278,,"      Path file1 = fs.makeQualified(new Path(""filelocal.dat""));"
101703,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java,118,,"    int exitCode = admin.run(new String[] {""-safemode"", ""enter""});"
101704,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,65,,"  private static final String NSID = ""ns1"";"
101705,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,94,,"    conf.set(DFSConfigKeys.DFS_HA_NAMENODE_ID_KEY, ""nn1"");"
101706,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,99,,"            DFSConfigKeys.DFS_NAMENODE_RPC_ADDRESS_KEY, NSID, ""nn2""),"
101707,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,151,,"    assertEquals(-1, runTool(""-ns""));"
101708,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,163,,"    assertEquals(0, runTool(""-getServiceState"", ""nn1""));"
101709,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,269,,"    assertEquals(-1, runTool(""-failover"", ""nn1"", ""nn2""));"
101710,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java,296,,"    assertEquals(0, runTool(""-failover"", ""nn1"", ""nn2"", ""--forcefence""));"
101711,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java,90,,"    assertEquals(0, runTool(""-getServiceState"", ""nn1""));"
101712,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java,90,,"    assertEquals(0, runTool(""-getServiceState"", ""nn1""));"
101713,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java,91,,"    assertEquals(0, runTool(""-getServiceState"", ""nn2""));"
101714,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java,104,,"    assertEquals(0, runTool(""-transitionToActive"", ""nn1""));"
101715,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java,124,,"    assertEquals(-1, runTool(""-failover"", ""nn2"", ""nn1""));"
101716,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdminMiniCluster.java,166,,"    assertEquals(0, runTool(""-failover"", ""nn1"", ""nn2"", ""--forcefence""));"
101717,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetConf.java,283,,"    verifyAddresses(conf, TestType.NAMENODE, false, ""localhost:1000"");"
101718,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestGetConf.java,287,,"    conf.set(DFS_NAMENODE_BACKUP_ADDRESS_KEY,""localhost:1001"");"
101719,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java,104,,"    assertEquals(0, runOev(edits, editsParsedXml, ""xml"", false));"
101720,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/TestOfflineImageViewer.java,292,,"      URL url = new URL(""http://localhost:"" + port +"
101721,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestDiff.java,130,,"        System.out.println(""previous = "" + previous);"
101722,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/util/TestDiff.java,132,,"        System.out.println(""current  = "" + current);"
101723,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestJsonUtil.java,84,,"    response.put(""ipAddr"", ""127.0.0.1"");"
101724,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestJsonUtil.java,112,,"    response.put(""name"", name);"
101725,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestTokenAspect.java,183,,"    fs.initialize(new URI(""dummyfs://127.0.0.1:1234""), conf);"
101726,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java,179,,"    final Ticker t = new Ticker(""SEEK"", ""offset=%d, remaining=%d"","
101727,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java,352,,"      final Path foo = new Path(""/foo"");"
101728,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsFileSystemContract.java,304,,"      testFileOut.write(content.getBytes(""US-ASCII""));"
101729,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTimeouts.java,116,,"      fail(""expected timeout"");"
101730,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTimeouts.java,118,,"      assertEquals(""connect timed out"", e.getMessage());"
101731,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTimeouts.java,131,,"      assertEquals(""Read timed out"", e.getMessage());"
101732,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTimeouts.java,172,,"      fs.getFileChecksum(new Path(""/file""));"
101733,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsTokens.java,159,,"  @SuppressWarnings(""unchecked"") // for any(Token.class)"
101734,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHdfsUrl.java,84,,"        UserGroupInformation.createRemoteUser(""test-user"");"
101735,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/resources/TestParam.java,59,,"      LOG.info(""EXPECTED: "" + e);"
101736,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/resources/TestParam.java,115,,"      new DestinationParam(""abc"");"
101737,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/net/TestNetworkTopology.java,51,,"        DFSTestUtil.getDatanodeDescriptor(""1.1.1.1"", ""/d1/r1""),"
101738,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/net/TestNetworkTopology.java,64,,"        DFSTestUtil.getDatanodeDescriptor(""14.14.14.14"", ""/d4/r1""),"
101739,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermissionSymlinks.java,160,,"      GenericTestUtils.assertExceptionContains(""Permission denied"", e);"
101740,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermissionSymlinks.java,277,,"        Path newlink = new Path(linkParent, ""newlink"");"
101741,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java,160,,"    config.set(userKeyHosts,""127.0.0.1"");"
101742,./TargetProjects/hadoop-common/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestRefreshUserMappings.java,189,,"      System.err.println(""auth for "" + ugi1.getUserName() + "" failed"");"
101743,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java,700,,"          LOG.error(""Can't make a speculator -- check """
101744,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java,1028,,"  @SuppressWarnings(""unchecked"")"
101745,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java,284,,"    @SuppressWarnings(""unchecked"")"
101746,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java,290,,"      String message = ""Kill job "" + jobId + "" received from "" + callerUGI"
101747,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/client/MRClientService.java,291,,"          + "" at "" + Server.getRemoteAddress();"
101748,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/commit/CommitterEventHandler.java,251,,"    @SuppressWarnings(""unchecked"")"
101749,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java,1060,,"  @SuppressWarnings(""unchecked"")"
101750,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java,1355,,"            taskAttempt.container == null ? ""UNKNOWN"""
101751,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java,1604,,"        LOG.debug(""Not generating HistoryFinish event since start event not "" +"
101752,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java,1605,,"            ""generated for taskAttempt: "" + taskAttempt.getID());"
101753,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java,210,,"      @SuppressWarnings(""unchecked"")"
101754,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/DefaultSpeculator.java,137,,"      LOG.error(""Can't make a speculation runtime extimator"", ex);"
101755,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java,250,,"      @PathParam(""jobid"") String jid) {"
101756,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AMWebServices.java,332,,"      @PathParam(""jobid"") String jid, @PathParam(""taskid"") String tid) {"
101757,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java,178,,"    set(COUNTER_GROUP, URLDecoder.decode($(COUNTER_GROUP), ""UTF-8""));"
101758,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JobBlock.java,136,,"            td().a(url(""tasks"", jid, ""m"", ""ALL""),String.valueOf(jinfo.getMapsTotal()))._()."
101759,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JobBlock.java,163,,"          td().a(url(""attempts"", jid, ""m"","
101760,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JobConfPage.java,48,,"    set(DATATABLES_ID, ""conf"");"
101761,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JobConfPage.java,78,,"    ""} );\n""+"
101762,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/NavBlock.java,45,,"          li().a(url(rmweb, ""cluster"", ""cluster""), ""About"")._()."
101763,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TaskPage.java,85,,"        .append(ta.getId()).append(""\"",\"""")"
101764,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/TasksBlock.java,107,,"              StringEscapeUtils.escapeHtml(info.getStatus()))).append(""\"",\"""")"
101765,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapred/TestTaskAttemptListenerImpl.java,231,,"    JobID jid = new JobID(""12345"", 1);"
101766,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestEvents.java,50,,"    JobID jid = new JobID(""001"", 1);"
101767,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestEvents.java,55,,"        TaskType.REDUCE, ""TEST"", 123L, ""RAKNAME"", ""HOSTNAME"", ""STATUS"","
101768,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestEvents.java,134,,"    assertEquals(""task_1_2_r03_4"","
101769,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestEvents.java,229,,"    datum.attemptId = ""attempt_1_2_r3_4_5"";"
101770,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/jobhistory/TestJobHistoryEventHandler.java,116,,"          t.appAttemptId, 200, t.containerId, ""nmhost"", 3000, 4000)));"
101771,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java,335,,"          + "" Waiting for Internal state : "" + finalState + ""   progress : """
101772,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/MRApp.java,371,,"          "" Waiting for state : "" + finalState +"
101773,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFetchFailure.java,64,,"    Assert.assertEquals(""Num tasks not correct"","
101774,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFetchFailure.java,85,,"    Assert.assertEquals(""Num completion events not correct"","
101775,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFetchFailure.java,87,,"    Assert.assertEquals(""Event status not correct"","
101776,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFetchFailure.java,138,,"    Assert.assertEquals(""Event map attempt id not correct"","
101777,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFetchFailure.java,146,,"    Assert.assertEquals(""Event status not correct for map attempt1"","
101778,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestFetchFailure.java,158,,"    Assert.assertEquals(""Incorrect number of map events"", 2, mapEvents.length);"
101779,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKill.java,84,,"    Assert.assertEquals(""Task state not correct"", TaskState.KILLED, "
101780,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKill.java,88,,"    Assert.assertEquals(""No of attempts is not correct"", 1, "
101781,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKill.java,91,,"    Assert.assertEquals(""Attempt state not correct"", TaskAttemptState.KILLED, "
101782,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRApp.java,99,,"    Assert.assertEquals(""Num tasks not correct"", 1, job.getTasks().size());"
101783,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java,115,,"    String userName = ""TestAppMasterUser"";"
101784,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java,120,,"        new MRAppMasterTest(applicationAttemptId, containerId, ""host"", -1, -1,"
101785,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java,134,,"    String applicationAttemptIdStr = ""appattempt_1317529182569_0004_000002"";"
101786,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java,135,,"    String containerIdStr = ""container_1317529182569_0004_000002_1"";"
101787,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java,156,,"      LOG.info(""Caught expected Exception"", e);"
101788,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestMRAppMaster.java,368,,"    credentials.addSecretKey(keyAlias, ""mySecretKey"".getBytes());"
101789,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,132,,"    conf.setBoolean(""mapred.mapper.new-api"", true);"
101790,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,133,,"    conf.setBoolean(""mapred.reducer.new-api"", true);"
101791,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,140,,"    Assert.assertEquals(""No of tasks not correct"","
101792,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,160,,"    Assert.assertEquals(""Reduce Task state not correct"","
101793,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,175,,"      LOG.info(""Waiting for next attempt to start"");"
101794,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,442,,"            ""want.am.recovery"", false);"
101795,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRecovery.java,462,,"    conf.setClass(""mapred.output.committer.class"","
101796,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRuntimeEstimators.java,299,,"      throw new UnsupportedOperationException(""Not supported yet."");"
101797,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestJobImpl.java,529,,"    JobID jobID = JobID.forName(""job_1234567890000_0001"");"
101798,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java,73,,"      app.cmdEnvironment.containsKey(""HADOOP_ROOT_LOGGER""));"
101799,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestMapReduceChildJVM.java,77,,"      app.cmdEnvironment.containsKey(""HADOOP_CLIENT_OPTS""));"
101800,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,345,,"    when(taListener.getAddress()).thenReturn(new InetSocketAddress(""localhost"", 0));"
101801,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,348,,"    jobConf.setClass(""fs.file.impl"", StubbedFS.class, FileSystem.class);"
101802,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,349,,"    jobConf.setBoolean(""fs.file.impl.disable.cache"", true);"
101803,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,354,,"    when(splits.getLocations()).thenReturn(new String[] {""127.0.0.1""});"
101804,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,418,,"    NodeId nid = NodeId.newInstance(""127.0.0.2"", 0);"
101805,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,423,,"    when(container.getNodeHttpAddress()).thenReturn(""localhost:0"");"
101806,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TestTaskAttempt.java,434,,"    assertFalse(""InternalError occurred trying to handle TA_CONTAINER_CLEANED"","
101807,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/launcher/TestContainerLauncher.java,121,,"        containerId, ""host"" + i + "":1234"", null,"
101808,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/launcher/TestContainerLauncherImpl.java,151,,"    @SuppressWarnings(""rawtypes"")"
101809,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/launcher/TestContainerLauncherImpl.java,154,,"    String cmAddress = ""127.0.0.1:8000"";"
101810,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/launcher/TestContainerLauncherImpl.java,171,,"      LOG.info(""inserting launch event"");"
101811,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,157,,"    MockNM amNodeManager = rm.registerNode(""amNM:1234"", 2048);"
101812,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,169,,"        MRBuilderUtils.newJobReport(jobId, ""job"", ""user"", JobState.RUNNING, 0, "
101813,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,169,,"        MRBuilderUtils.newJobReport(jobId, ""job"", ""user"", JobState.RUNNING, 0, "
101814,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,170,,"            0, 0, 0, 0, 0, 0, ""jobfile"", null, false, """"));"
101815,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,175,,"    MockNM nodeManager1 = rm.registerNode(""h1:1234"", 10240);"
101816,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,176,,"    MockNM nodeManager2 = rm.registerNode(""h2:1234"", 10240);"
101817,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,177,,"    MockNM nodeManager3 = rm.registerNode(""h3:1234"", 10240);"
101818,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,194,,"    Assert.assertEquals(""No of assignments must be 0"", 0, assigned.size());"
101819,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/rm/TestRMContainerAllocator.java,1148,,"    Assert.assertEquals(""No of assignments must be 1"", 1, assigned.size());"
101820,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServices.java,114,,"    ClientResponse response = r.path(""ws"").path(""v1"").path(""mapreduce"")"
101821,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServices.java,118,,"    assertEquals(""incorrect number of elements"", 1, json.length());"
101822,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServices.java,119,,"    verifyAMInfo(json.getJSONObject(""info""), appContext);"
101823,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java,130,,"        ClientResponse response = r.path(""ws"").path(""v1"").path(""mapreduce"")"
101824,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java,131,,"            .path(""jobs"").path(jobId).path(""tasks"").path(tid).path(""attempts"")"
101825,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java,131,,"            .path(""jobs"").path(jobId).path(""tasks"").path(tid).path(""attempts"")"
101826,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java,131,,"            .path(""jobs"").path(jobId).path(""tasks"").path(tid).path(""attempts"")"
101827,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java,199,,"        assertEquals(""incorrect number of elements"", 1, attempts.getLength());"
101828,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesAttempts.java,201,,"        NodeList nodes = dom.getElementsByTagName(""taskAttempt"");"
101829,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobConf.java,165,,"      ClientResponse response = r.path(""ws"").path(""v1"").path(""mapreduce"")"
101830,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobConf.java,166,,"          .path(""jobs"").path(jobId).path(""conf"")"
101831,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobConf.java,166,,"          .path(""jobs"").path(jobId).path(""conf"")"
101832,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobConf.java,170,,"      assertEquals(""incorrect number of elements"", 1, json.length());"
101833,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobConf.java,237,,"    WebServicesTestUtils.checkStringMatch(""path"", job.getConfFile().toString(),"
101834,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,126,,"    ClientResponse response = r.path(""ws"").path(""v1"").path(""mapreduce"")"
101835,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,127,,"        .path(""jobs"").accept(MediaType.APPLICATION_JSON)"
101836,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,131,,"    assertEquals(""incorrect number of elements"", 1, json.length());"
101837,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,133,,"    JSONArray arr = jobs.getJSONArray(""job"");"
101838,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,256,,"      fail(""should have thrown exception on invalid uri"");"
101839,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,262,,"      JSONObject exception = msg.getJSONObject(""RemoteException"");"
101840,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,264,,"      String message = exception.getString(""message"");"
101841,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,265,,"      String type = exception.getString(""exception"");"
101842,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,266,,"      String classname = exception.getString(""javaClassName"");"
101843,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,420,,"        info.getString(""name""), info.getString(""state""),"
101844,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,421,,"        info.getLong(""startTime""), info.getLong(""finishTime""),"
101845,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,429,,"    if (info.has(""diagnostics"")) {"
101846,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,460,,"              WebServicesTestUtils.checkStringMatch(""value"", expectValue,"
101847,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,638,,"      JSONObject info = json.getJSONObject(""jobCounters"");"
101848,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesJobs.java,790,,"      JSONObject info = json.getJSONObject(""jobAttempts"");"
101849,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,123,,"      ClientResponse response = r.path(""ws"").path(""v1"").path(""mapreduce"")"
101850,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,124,,"          .path(""jobs"").path(jobId).path(""tasks"")"
101851,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,124,,"          .path(""jobs"").path(jobId).path(""tasks"")"
101852,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,128,,"      assertEquals(""incorrect number of elements"", 1, json.length());"
101853,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,130,,"      JSONArray arr = tasks.getJSONArray(""task"");"
101854,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,208,,"          .path(""jobs"").path(jobId).path(""tasks"").queryParam(""type"", type)"
101855,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,253,,"        fail(""should have thrown exception on invalid uri"");"
101856,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,259,,"        JSONObject exception = msg.getJSONObject(""RemoteException"");"
101857,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,261,,"        String message = exception.getString(""message"");"
101858,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,262,,"        String type = exception.getString(""exception"");"
101859,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,263,,"        String classname = exception.getString(""javaClassName"");"
101860,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,264,,"        WebServicesTestUtils.checkStringMatch(""exception message"","
101861,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,266,,"        WebServicesTestUtils.checkStringMatch(""exception type"","
101862,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,268,,"        WebServicesTestUtils.checkStringMatch(""exception classname"","
101863,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,362,,"            ""NotFoundException"", type);"
101864,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,364,,"            ""org.apache.hadoop.yarn.webapp.NotFoundException"", classname);"
101865,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAMWebServicesTasks.java,626,,"        JSONObject info = json.getJSONObject(""jobTaskCounters"");"
101866,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java,62,,"    JobId jobID = MRApps.toJobID(""job_01_01"");"
101867,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java,100,,"        appController.getProperty().get(""title""));"
101868,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/webapp/TestAppController.java,140,,"        ""Access denied: User user does not have permission to view job job_01_01"","
101869,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/FileNameIndexUtils.java,131,,"            + jhFileName + "" : "" + e);"
101870,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/JHAdminConfig.java,39,,"  public static final String DEFAULT_MR_HISTORY_ADDRESS = ""0.0.0.0:"" +"
101871,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestLocalDistributedCacheManager.java,142,,"        if(""file.txt"".equals(p.getName())) {"
101872,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapred/TestLocalDistributedCacheManager.java,146,,"          throw new FileNotFoundException(p+"" not supported by mocking"");"
101873,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/v2/util/TestMRApps.java,195,,"    assertTrue(environment.get(""CLASSPATH"").startsWith("
101874,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/v2/util/TestMRApps.java,263,,"      Arrays.asList(ApplicationConstants.Environment.PWD.$$(), ""job.jar/job.jar"","
101875,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/v2/util/TestMRApps.java,264,,"        ""job.jar/classes/"", ""job.jar/lib/*"","
101876,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/v2/util/TestMRApps.java,264,,"        ""job.jar/classes/"", ""job.jar/lib/*"","
101877,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/v2/util/TestMRApps.java,382,,"    conf.set(MRJobConfig.CACHE_ARCHIVES_VISIBILITIES, ""true"");"
101878,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IFile.java,202,,"                              "" for "" + key);"
101879,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MROutputFiles.java,131,,"    return lDirAlloc.getLocalPathToRead(MRJobConfig.OUTPUT + ""/spill"""
101880,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java,288,,"    @SuppressWarnings(""unchecked"")"
101881,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java,1977,,"        LOG.info(""Ignoring exception during close for "" + c, ie);"
101882,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java,303,,"     @SuppressWarnings(""unchecked"")"
101883,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java,1475,,"  @SuppressWarnings(""unchecked"")"
101884,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/join/CompositeRecordReader.java,69,,"  @SuppressWarnings(""unchecked"") // Generic array assignment"
101885,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleInputs.java,76,,"    String mappers = conf.get(""mapreduce.input.multipleinputs.dir.mappers"");"
101886,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/MultipleOutputs.java,455,,"    @SuppressWarnings({""unchecked""})"
101887,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/Submitter.java,403,,"                  ""class"");"
101888,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/FileSystemCounterGroup.java,204,,"  @SuppressWarnings(""unchecked"")"
101889,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/FrameworkCounterGroup.java,122,,"  @SuppressWarnings(""unchecked"")"
101890,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java,254,,"    taskSummary.append(""\t\t"").append(ts.numFailedSetups);"
101891,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java,300,,"    printAnalysis(avg.getMapTasks(), cMap, ""map"", avg.getAvgMapTime(), 10);"
101892,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java,304,,"      printAnalysis(avg.getReduceTasks(), cShuffle, ""shuffle"", "
101893,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/chain/Chain.java,79,,"  @SuppressWarnings(""unchecked"")"
101894,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/BooleanSplitter.java,45,,"          colName + "" IS NULL"", colName + "" IS NULL""));"
101895,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/DateSplitter.java,72,,"          colName + "" IS NULL"", colName + "" IS NULL""));"
101896,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/FloatSplitter.java,59,,"          colName + "" IS NULL"", colName + "" IS NULL""));"
101897,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/IntegerSplitter.java,56,,"          colName + "" IS NULL"", colName + "" IS NULL""));"
101898,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/db/TextSplitter.java,88,,"          colName + "" IS NULL"", colName + "" IS NULL""));"
101899,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/MultipleInputs.java,54,,"  @SuppressWarnings(""unchecked"")"
101900,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/SequenceFileInputFilter.java,164,,"          ""Negative "" + FILTER_FREQUENCY + "": "" + frequency);"
101901,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input/TaggedInputSplit.java,50,,"  @SuppressWarnings(""unchecked"")"
101902,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/CompositeRecordReader.java,57,,"  @SuppressWarnings(""unchecked"")"
101903,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/Parser.java,193,,"@SuppressWarnings(""unchecked"")"
101904,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/join/WrappedRecordReader.java,56,,"  @SuppressWarnings(""unchecked"")"
101905,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java,337,,"     LOG.debug(""Merging data from ""+from+"" to ""+to);"
101906,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java,273,,"  @SuppressWarnings(""unchecked"")"
101907,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/partition/InputSampler.java,126,,"    @SuppressWarnings(""unchecked"") // ArrayList::toArray doesn't preserve type"
101908,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/JobContextImpl.java,171,,"  @SuppressWarnings(""unchecked"")"
101909,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java,138,,"    setName(""fetcher#"" + id);"
101910,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java,169,,"      if (argv.length != 2 && !(argv.length == 3 && ""all"".equals(argv[1]))) {"
101911,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java,266,,"          System.out.println(""Could not find job "" + jobid);"
101912,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestCounters.java,99,,"    String[] groups = {""group1"", ""group2"", ""group{}()[]""};"
101913,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestCounters.java,100,,"    String[] counters = {""counter1"", ""counter2"", ""counter{}()[]""};"
101914,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestCounters.java,100,,"    String[] counters = {""counter1"", ""counter2"", ""counter{}()[]""};"
101915,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestCounters.java,119,,"  @SuppressWarnings(""deprecation"")"
101916,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestCounters.java,275,,"      counters.findCounter(""test"", ""test"" + i);"
101917,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobAclsManager.java,42,,"    String jobOwner = ""testuser"";"
101918,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestJobAclsManager.java,46,,"    String clusterAdmin = ""testuser2"";"
101919,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestOldMethodsJobID.java,42,,"  @SuppressWarnings(""deprecation"")"
101920,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestOldMethodsJobID.java,51,,"    test = new TaskID(""001"", 1, false, 1);"
101921,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java,71,,"      manager.setSchedulerInfo(""first"", ""queueInfo"");"
101922,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java,71,,"      manager.setSchedulerInfo(""first"", ""queueInfo"");"
101923,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java,72,,"      manager.setSchedulerInfo(""second"", ""queueInfoqueueInfo"");"
101924,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java,72,,"      manager.setSchedulerInfo(""second"", ""queueInfoqueueInfo"");"
101925,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java,87,,"      assertEquals(firstSubQueue.getState().getStateName(), ""running"");"
101926,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestQueue.java,88,,"      assertEquals(secondSubQueue.getState().getStateName(), ""stopped"");"
101927,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/lib/db/TestDBInputFormat.java,106,,"    DBConfiguration.configureDB(jConfiguration, ""driverClass"", ""dbUrl"", ""user"","
101928,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/lib/db/TestDBInputFormat.java,106,,"    DBConfiguration.configureDB(jConfiguration, ""driverClass"", ""dbUrl"", ""user"","
101929,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/db/TestSplitters.java,56,,"    when(result.getString(1)).thenReturn(""result1"");"
101930,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/db/TestSplitters.java,58,,"    List<InputSplit> splits=splitter.split(configuration, result, ""column"");"
101931,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/db/TestSplitters.java,63,,"    when(result.getString(2)).thenReturn(""result2"");"
101932,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/db/TestSplitters.java,94,,"    splits = splitter.split(configuration, results, ""column1"");"
101933,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFileInputFormat.java,103,,"        ""test:/a1/file1""), splits);"
101934,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFileInputFormat.java,230,,"    Path base1 = new Path(TEST_ROOT_DIR, ""input1"");"
101935,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFileInputFormat.java,237,,"    Path in1File1 = new Path(base1, ""file1"");"
101936,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/lib/input/TestFileInputFormat.java,238,,"    Path in1File2 = new Path(base1, ""file2"");"
101937,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMergeManager.java,68,,"    Assert.assertTrue(""Should be a memory merge"","
101938,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java,118,,"    map1.put(""apple"", ""disgusting"");"
101939,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java,119,,"    map1.put(""carrot"", ""delicious"");"
101940,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java,121,,"    map1.put(""banana"", ""pretty good"");"
101941,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/tools/TestCLI.java,52,,"    int retCode_MAP = cli.run(new String[] { ""-list-attempt-ids"", jobIdStr,"
101942,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/tools/TestCLI.java,53,,"        ""MAP"", ""running"" });"
101943,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryClientService.java,283,,"      throw new IOException(""Invalid operation on completed job"");"
101944,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java,574,,"        LOG.info(""Waiting for FileSystem at "" +"
101945,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java,681,,"          + serialDirPath.toString() + "". Continuing with next"");"
101946,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsConfPage.java,47,,"    set(DATATABLES_ID, ""conf"");"
101947,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsConfPage.java,80,,"    ""} );\n""+"
101948,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobBlock.java,181,,"          td().a(url(""attempts"", jid, ""m"","
101949,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobsBlock.java,78,,"      .append(dateFormat.format(new Date(job.getSubmitTime()))).append(""\"",\"""")"
101950,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobsBlock.java,106,,"        th().input(""search_init"").$type(InputType.text).$name(""submit_time"").$value(""Submit Time"")._()._()."
101951,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsJobsBlock.java,107,,"        th().input(""search_init"").$type(InputType.text).$name(""start_time"").$value(""Start Time"")._()._()."
101952,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTaskPage.java,150,,"        .append(sortId + "" "").append(taid).append(""\"",\"""")"
101953,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTaskPage.java,190,,"          th().input(""search_init"").$type(InputType.text)."
101954,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTaskPage.java,261,,"    set(DATATABLES_ID, ""attempts"");"
101955,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTaskPage.java,320,,"           ""} );\n""+"
101956,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksBlock.java,86,,"            th(""Start Time"")."
101957,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksBlock.java,87,,"            th(""Finish Time"")."
101958,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksBlock.java,88,,"            th(""Elapsed Time"")."
101959,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksBlock.java,151,,"      .append(info.getState()).append(""\"",\"""")"
101960,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksBlock.java,179,,"    footRow.th().input(""search_init"").$type(InputType.text).$name(""task"")"
101961,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksPage.java,46,,"    set(DATATABLES_ID, ""tasks"");"
101962,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsTasksPage.java,100,,"           ""} );\n""+"
101963,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsView.java,43,,"    set(DATATABLES_ID, ""jobs"");"
101964,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsView.java,106,,"           ""} );\n""+"
101965,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsWebServices.java,157,,"        throw new BadRequestException(""Invalid number format: "" + e.getMessage());"
101966,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsWebServices.java,220,,"      @PathParam(""jobid"") String jid) {"
101967,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/webapp/HsWebServices.java,307,,"      @PathParam(""jobid"") String jid, @PathParam(""taskid"") String tid) {"
101968,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java,53,,"        new Text(""tokenOwner""), new Text(""tokenRenewer""),"
101969,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java,53,,"        new Text(""tokenOwner""), new Text(""tokenRenewer""),"
101970,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJHSDelegationTokenSecretManager.java,54,,"        new Text(""tokenUser""));"
101971,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryEntities.java,90,,"      new CompletedJob(conf, jobId, fulleHistoryPath, loadTasks, ""user"","
101972,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java,204,,"      Assert.assertEquals(""JobName does not match"", ""test"","
101973,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java,245,,"        LOG.info(""Can not open history file: "" + historyFilePath, ioe);"
101974,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java,246,,"        throw (new Exception(""Can not open History File""));"
101975,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/server/TestHSAdminServer.java,169,,"    conf.set(""hadoop.proxyuser.superuser.hosts"", ""127.0.0.1"");"
101976,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java,119,,"    ClientResponse response = r.path(""ws"").path(""v1"").path(""history"")"
101977,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java,123,,"    assertEquals(""incorrect number of elements"", 1, json.length());"
101978,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java,124,,"    verifyHSInfo(json.getJSONObject(""historyInfo""), appContext);"
101979,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAcls.java,104,,"      fail(""enemy can access job"");"
101980,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,140,,"        ClientResponse response = r.path(""ws"").path(""v1"").path(""history"")"
101981,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,141,,"            .path(""mapreduce"").path(""jobs"").path(jobId).path(""tasks"").path(tid)"
101982,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,141,,"            .path(""mapreduce"").path(""jobs"").path(jobId).path(""tasks"").path(tid)"
101983,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,141,,"            .path(""mapreduce"").path(""jobs"").path(jobId).path(""tasks"").path(tid)"
101984,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,142,,"            .path(""attempts"").accept(MediaType.APPLICATION_JSON)"
101985,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,212,,"        assertEquals(""incorrect number of elements"", 1, attempts.getLength());"
101986,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesAttempts.java,214,,"        NodeList nodes = dom.getElementsByTagName(""taskAttempt"");"
101987,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java,168,,"      ClientResponse response = r.path(""ws"").path(""v1"").path(""history"")"
101988,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java,169,,"          .path(""mapreduce"")"
101989,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java,170,,"          .path(""jobs"").path(jobId).path(""conf"")"
101990,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java,170,,"          .path(""jobs"").path(jobId).path(""conf"")"
101991,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java,174,,"      assertEquals(""incorrect number of elements"", 1, json.length());"
101992,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobConf.java,241,,"    WebServicesTestUtils.checkStringMatch(""path"", job.getConfFile().toString(),"
101993,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,131,,"    ClientResponse response = r.path(""ws"").path(""v1"").path(""history"")"
101994,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,132,,"        .path(""mapreduce"").path(""jobs"").accept(MediaType.APPLICATION_JSON)"
101995,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,132,,"        .path(""mapreduce"").path(""jobs"").accept(MediaType.APPLICATION_JSON)"
101996,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,136,,"    assertEquals(""incorrect number of elements"", 1, json.length());"
101997,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,138,,"    JSONArray arr = jobs.getJSONArray(""job"");"
101998,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,215,,"          WebServicesTestUtils.getXmlString(element, ""name""),"
101999,./TargetProjects/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServicesJobs.java,218,,"          WebServicesTestUtils.getXmlLong(element, ""startTime""),"
